{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/yanghq/utils/anaconda3/envs/glm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "d=\"/data/yanghq/models/YHQ/chatglm-6b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    }
   ],
   "source": [
    "# 载入Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    d, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mquantize(\u001b[38;5;241m8\u001b[39m)\u001b[38;5;241m.\u001b[39mhalf()\u001b[38;5;241m.\u001b[39mcuda()\n",
      "File \u001b[0;32m~/utils/anaconda3/envs/glm/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:466\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    462\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m get_class_from_dynamic_module(\n\u001b[1;32m    463\u001b[0m         pretrained_model_name_or_path, module_file \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.py\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    464\u001b[0m     )\n\u001b[1;32m    465\u001b[0m     model_class\u001b[38;5;241m.\u001b[39mregister_for_auto_class(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m--> 466\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    470\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n",
      "File \u001b[0;32m~/utils/anaconda3/envs/glm/lib/python3.9/site-packages/transformers/modeling_utils.py:2498\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2495\u001b[0m     init_contexts\u001b[38;5;241m.\u001b[39mappend(init_empty_weights())\n\u001b[1;32m   2497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[0;32m-> 2498\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2500\u001b[0m \u001b[38;5;66;03m# Check first if we are `from_pt`\u001b[39;00m\n\u001b[1;32m   2501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_keep_in_fp32_modules:\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/chatglm-6b/modeling_chatglm.py:1049\u001b[0m, in \u001b[0;36mChatGLMForConditionalGeneration.__init__\u001b[0;34m(self, config, empty_init)\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_sequence_length \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mmax_sequence_length\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_encoding_2d \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mposition_encoding_2d\n\u001b[0;32m-> 1049\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer \u001b[38;5;241m=\u001b[39m \u001b[43mChatGLMModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mempty_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mempty_init\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head \u001b[38;5;241m=\u001b[39m init_method(\n\u001b[1;32m   1052\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLinear,\n\u001b[1;32m   1053\u001b[0m     config\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1056\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mhalf\n\u001b[1;32m   1057\u001b[0m )\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/chatglm-6b/modeling_chatglm.py:857\u001b[0m, in \u001b[0;36mChatGLMModel.__init__\u001b[0;34m(self, config, empty_init)\u001b[0m\n\u001b[1;32m    855\u001b[0m     param\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprefix_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_seq_len)\u001b[38;5;241m.\u001b[39mlong()\n\u001b[0;32m--> 857\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprefix_encoder \u001b[38;5;241m=\u001b[39m \u001b[43mPrefixEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mDropout(\u001b[38;5;241m0.1\u001b[39m)\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/chatglm-6b/modeling_chatglm.py:156\u001b[0m, in \u001b[0;36mPrefixEncoder.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrans \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m    151\u001b[0m         torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLinear(config\u001b[38;5;241m.\u001b[39mhidden_size, config\u001b[38;5;241m.\u001b[39mhidden_size),\n\u001b[1;32m    152\u001b[0m         torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mTanh(),\n\u001b[1;32m    153\u001b[0m         torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLinear(config\u001b[38;5;241m.\u001b[39mhidden_size, config\u001b[38;5;241m.\u001b[39mnum_layers \u001b[38;5;241m*\u001b[39m config\u001b[38;5;241m.\u001b[39mhidden_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    154\u001b[0m     )\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_seq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/utils/anaconda3/envs/glm/lib/python3.9/site-packages/torch/nn/modules/sparse.py:143\u001b[0m, in \u001b[0;36mEmbedding.__init__\u001b[0;34m(self, num_embeddings, embedding_dim, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse, _weight, _freeze, device, dtype)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_grad_by_freq \u001b[38;5;241m=\u001b[39m scale_grad_by_freq\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    144\u001b[0m                             requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m _freeze)\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_parameters()\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:\n * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.memory_format memory_format, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\n",
    "    d, trust_remote_code=True).quantize(8).half().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response, history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mchat(tokenizer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m我是谁？\u001b[39m\u001b[38;5;124m\"\u001b[39m, history\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "response, history = model.chat(tokenizer, \"我是谁？\", history=[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatGLMForConditionalGeneration(\n",
      "  (transformer): ChatGLMModel(\n",
      "    (word_embeddings): Embedding(130528, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x GLMBlock(\n",
      "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "        (attention): SelfAttention(\n",
      "          (rotary_emb): RotaryEmbedding()\n",
      "          (query_key_value): QuantizedLinear(in_features=4096, out_features=12288, bias=True)\n",
      "          (dense): QuantizedLinear(in_features=4096, out_features=4096, bias=True)\n",
      "        )\n",
      "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GLU(\n",
      "          (dense_h_to_4h): QuantizedLinear(in_features=4096, out_features=16384, bias=True)\n",
      "          (dense_4h_to_h): QuantizedLinear(in_features=16384, out_features=4096, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=130528, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.transformer.pre_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 3 µs, total: 8 µs\n",
      "Wall time: 21.2 µs\n",
      "tensor([2, 4, 6, 8])\n",
      "tensor([ 1,  3,  5,  7,  9, 11, 13, 15, 17])\n",
      "tensor([ 1,  3,  5,  7,  6,  2,  9, 11, 13, 15,  8,  4, 17])\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "# 假设b和c的形状分别是[4, C, H, W]和[6, C, H, W]（C, H, W代表其他维度）\n",
    "#b = torch.randn(4, 4096, 128, 256)  # 随机生成b\n",
    "#c = torch.randn(6, 4096, 128, 256)  # 随机生成c\n",
    "b = torch.arange(2,10,2)  # 随机生成b\n",
    "c = torch.arange(1,19,2)  # 随机生成c\n",
    "print(b)\n",
    "print(c)\n",
    "# 计算合并后的总长度\n",
    "total_length = b.size(0) + c.size(0)\n",
    "\n",
    "# 生成b的插入位置，确保b和c的内部顺序不变\n",
    "positions_b = torch.randperm(total_length)[:b.size(0)]\n",
    "\n",
    "# 生成一个足够大的张量来容纳结果\n",
    "result = torch.empty(total_length, *b.shape[1:], dtype=b.dtype)\n",
    "\n",
    "# 将b和c的元素放入正确位置\n",
    "result[positions_b] = b\n",
    "positions_c = torch.tensor([i for i in range(total_length) if i not in positions_b.tolist()])\n",
    "result[positions_c] = c\n",
    "\n",
    "print(result)\n",
    "# 结果张量result现在是b和c的随机合并，且内部顺序保持不变im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.7 s, sys: 8.98 s, total: 12.7 s\n",
      "Wall time: 1.15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "past_key   past_value\n",
    "key_layer  value_layer\n",
    "\n",
    "total_length = past_key.size(0) + key_layer.size(0)\n",
    "positions_b = torch.randperm(total_length)[:past_key.size(0)].to(dtype=torch.long)\n",
    "result = torch.empty(total_length, *b.shape[1:], dtype=past_key.dtype)\n",
    "positions_c = torch.ones(total_length, dtype=torch.bool)\n",
    "positions_c[positions_b] = False\n",
    "result[positions_b] = b\n",
    "result[positions_c] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([ 2,  1,  4,  3,  6,  5,  7,  9, 11, 13, 15, 17,  8]), tensor([ 0,  2,  4, 12]))\n"
     ]
    }
   ],
   "source": [
    "def distribute_insert_tensor(insert_tensor, large_tensor,insert_tensor_2,large_tensor_2, dim=0):\n",
    "    large_length = large_tensor.size(dim)\n",
    "    insert_length = insert_tensor.size(dim)\n",
    "    total_length = large_length + insert_length\n",
    "    indices_insert = torch.randperm(total_length)[:insert_length]\n",
    "    indices_insert = indices_insert.sort()[0]\n",
    "    indices_large = torch.ones(total_length, dtype=torch.bool)\n",
    "    indices_large[indices_insert] = False\n",
    "    result_tensor = torch.empty(total_length, dtype=insert_tensor.dtype, device=insert_tensor.device)\n",
    "    result_tensor[indices_insert] = insert_tensor\n",
    "    result_tensor[indices_large] = large_tensor\n",
    "    result_tensor_2 = torch.empty(total_length, dtype=insert_tensor.dtype, device=insert_tensor.device)\n",
    "    result_tensor_2[indices_insert] = insert_tensor_2\n",
    "    result_tensor_2[indices_large] = large_tensor_2\n",
    "    return result_tensor,result_tensor_2\n",
    "b1 = torch.arange(2,10,2)  # 随机生成b\n",
    "c1 = torch.arange(1,19,2)  # 随机生成c\n",
    "new_key_layer = distribute_insert_tensor(b1, c1, dim=0)\n",
    "print(new_key_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 4, 6, 8])\n",
      "tensor([ 1,  3,  5,  7,  9, 11, 13, 15, 17])\n"
     ]
    }
   ],
   "source": [
    "b1 = torch.arange(2,10,2)  # 随机生成b\n",
    "c1 = torch.arange(1,19,2)  # 随机生成c\n",
    "print(b)\n",
    "print(c)\n",
    "b = torch.randn(128, 1024, 16, 64)  # 随机生成b\n",
    "c = torch.randn(2048, 1024, 16, 64)  # 随机生成c\n",
    "\n",
    "def dispersed_insert(insert_tensor, large_tensor, dim=0):\n",
    "    total_length = large_tensor.size(dim) + insert_tensor.size(dim)\n",
    "    insert_positions = torch.randperm(total_length)[:insert_tensor.size(dim)]\n",
    "    print('',insert_positions)\n",
    "    result_tensor = torch.zeros(total_length, dtype=insert_tensor.dtype, device=insert_tensor.device)\n",
    "    large_index, insert_index = 0, 0\n",
    "    for i in range(total_length):\n",
    "        if i in insert_positions:\n",
    "            result_tensor[i] = insert_tensor[insert_index]\n",
    "            insert_index += 1\n",
    "        else:\n",
    "            result_tensor[i] = large_tensor[large_index]\n",
    "            large_index += 1\n",
    "    return result_tensor\n",
    "\n",
    "# 示例使用\n",
    "# 假设 past_key 和 key_layer 是预先定义的大型4维张量\n",
    "# 使用上面定义的函数进行操作\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[128], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m a\u001b[38;5;241m=\u001b[39mb\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m----> 2\u001b[0m a\u001b[38;5;241m=\u001b[39m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_length\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m+\u001b[39ma[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m      3\u001b[0m a\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "a=b.shape\n",
    "a=torch.Size(total_length)+a[1:]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(total_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
