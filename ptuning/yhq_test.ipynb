{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/yanghq/utils/anaconda3/envs/glm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    }
   ],
   "source": [
    "# 载入Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"/data/yanghq/models/THUDM/chatglm-6b\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:13<00:00,  1.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-quantize\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "model = AutoModel.from_pretrained(\n",
    "    \"/data/yanghq/models/THUDM/chatglm-6b\", trust_remote_code=True).quantize(8).half().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The dtype of attention mask (torch.int64) is not bool\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatGLMForConditionalGeneration - testtesttest\n",
      "ChatGLMModel - testtesttest\n",
      "ChatGLMForConditionalGeneration - testtesttest\n",
      "ChatGLMModel - testtesttest\n",
      "ChatGLMForConditionalGeneration - testtesttest\n",
      "ChatGLMModel - testtesttest\n",
      "ChatGLMForConditionalGeneration - testtesttest\n",
      "ChatGLMModel - testtesttest\n",
      "ChatGLMForConditionalGeneration - testtesttest\n",
      "ChatGLMModel - testtesttest\n",
      "ChatGLMForConditionalGeneration - testtesttest\n",
      "ChatGLMModel - testtesttest\n",
      "ChatGLMForConditionalGeneration - testtesttest\n",
      "ChatGLMModel - testtesttest\n",
      "ChatGLMForConditionalGeneration - testtesttest\n",
      "ChatGLMModel - testtesttest\n",
      "ChatGLMForConditionalGeneration - testtesttest\n",
      "ChatGLMModel - testtesttest\n",
      "ChatGLMForConditionalGeneration - testtesttest\n",
      "ChatGLMModel - testtesttest\n",
      "ChatGLMForConditionalGeneration - testtesttest\n",
      "ChatGLMModel - testtesttest\n",
      "ChatGLMForConditionalGeneration - testtesttest\n",
      "ChatGLMModel - testtesttest\n",
      "ChatGLMForConditionalGeneration - testtesttest\n",
      "ChatGLMModel - testtesttest\n",
      "ChatGLMForConditionalGeneration - testtesttest\n",
      "ChatGLMModel - testtesttest\n",
      "ChatGLMForConditionalGeneration - testtesttest\n",
      "ChatGLMModel - testtesttest\n",
      "ChatGLMForConditionalGeneration - testtesttest\n",
      "ChatGLMModel - testtesttest\n",
      "ChatGLMForConditionalGeneration - testtesttest\n",
      "ChatGLMModel - testtesttest\n",
      "ChatGLMForConditionalGeneration - testtesttest\n",
      "ChatGLMModel - testtesttest\n",
      "ChatGLMForConditionalGeneration - testtesttest\n",
      "ChatGLMModel - testtesttest\n",
      "ChatGLMForConditionalGeneration - testtesttest\n",
      "ChatGLMModel - testtesttest\n",
      "ChatGLMForConditionalGeneration - testtesttest\n",
      "ChatGLMModel - testtesttest\n",
      "ChatGLMForConditionalGeneration - testtesttest\n",
      "ChatGLMModel - testtesttest\n",
      "ChatGLMForConditionalGeneration - testtesttest\n",
      "ChatGLMModel - testtesttest\n",
      "ChatGLMForConditionalGeneration - testtesttest\n",
      "ChatGLMModel - testtesttest\n",
      "ChatGLMForConditionalGeneration - testtesttest\n",
      "ChatGLMModel - testtesttest\n",
      "ChatGLMForConditionalGeneration - testtesttest\n",
      "ChatGLMModel - testtesttest\n",
      "你好👋！我是人工智能助手 ChatGLM-6B，很高兴见到你，欢迎问我任何问题。\n"
     ]
    }
   ],
   "source": [
    "response, history = model.chat(tokenizer, \"你好\", history=[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatGLMForConditionalGeneration(\n",
      "  (transformer): ChatGLMModel(\n",
      "    (word_embeddings): Embedding(130528, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x GLMBlock(\n",
      "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "        (attention): SelfAttention(\n",
      "          (rotary_emb): RotaryEmbedding()\n",
      "          (query_key_value): QuantizedLinear(in_features=4096, out_features=12288, bias=True)\n",
      "          (dense): QuantizedLinear(in_features=4096, out_features=4096, bias=True)\n",
      "        )\n",
      "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GLU(\n",
      "          (dense_h_to_4h): QuantizedLinear(in_features=4096, out_features=16384, bias=True)\n",
      "          (dense_4h_to_h): QuantizedLinear(in_features=16384, out_features=4096, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=130528, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/data/yanghq/utils/anaconda3/envs/glm/lib/python3.9/site-packages/transformers']\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__path__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
